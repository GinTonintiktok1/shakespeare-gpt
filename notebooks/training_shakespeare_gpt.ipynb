{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPCR3n59poeMLxdAnUFdV1V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"collapsed":true,"id":"pigmkkYUh_zZ","executionInfo":{"status":"error","timestamp":1766017854753,"user_tz":-60,"elapsed":7963,"user":{"displayName":"Christian Laino","userId":"07149888719448262825"}},"outputId":"687d4e40-9217-45f5-ad29-32a854594e4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2095045459.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# ========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Directory checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["# ========================================\n","# VERSIONE SICURA - CON CHECKPOINT\n","# ========================================\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import math\n","import os\n","\n","# Device\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"Using device: {device}\")\n","\n","# ========================================\n","# MOUNT GOOGLE DRIVE (ESSENZIALE!)\n","# ========================================\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Directory checkpoint\n","checkpoint_dir = '/content/drive/MyDrive/shakespeare_checkpoints'\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","print(f\"‚úÖ Checkpoint directory: {checkpoint_dir}\")\n","\n","# ========================================\n","# HYPERPARAMETERS\n","# ========================================\n","vocab_size = 256\n","d_model = 128\n","num_heads = 8\n","num_layers = 6\n","d_ff = 512\n","max_seq_len = 128\n","dropout = 0.1\n","\n","batch_size = 32\n","learning_rate = 3e-4\n","num_epochs = 6  # ‚Üê RIDOTTO a 6 per sicurezza\n","warmup_steps = 100\n","\n","temperature = 0.8\n","top_k = 40\n","\n","# ========================================\n","# DATASET\n","# ========================================\n","class TextDataset(Dataset):\n","    def __init__(self, text, seq_len):\n","        self.data = torch.tensor([ord(c) for c in text], dtype=torch.long)\n","        self.seq_len = seq_len\n","\n","    def __len__(self):\n","        return len(self.data) - self.seq_len\n","\n","    def __getitem__(self, idx):\n","        x = self.data[idx:idx+self.seq_len]\n","        y = self.data[idx+1:idx+self.seq_len+1]\n","        return x, y\n","\n","# Scarica dataset\n","import requests\n","url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n","text = requests.get(url).text\n","print(f\"Dataset: {len(text)} characters\")\n","\n","# Split\n","split = int(0.9 * len(text))\n","train_text, val_text = text[:split], text[split:]\n","\n","train_dataset = TextDataset(train_text, max_seq_len)\n","val_dataset = TextDataset(val_text, max_seq_len)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# ========================================\n","# POSITIONAL ENCODING\n","# ========================================\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super().__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len).unsqueeze(1).float()\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n","                            -(math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        self.register_buffer('pe', pe.unsqueeze(0))\n","\n","    def forward(self, x):\n","        return x + self.pe[:, :x.size(1)]\n","\n","# ========================================\n","# MULTI-HEAD ATTENTION\n","# ========================================\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads, dropout=0.1):\n","        super().__init__()\n","        assert d_model % num_heads == 0\n","\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","\n","        self.W_q = nn.Linear(d_model, d_model)\n","        self.W_k = nn.Linear(d_model, d_model)\n","        self.W_v = nn.Linear(d_model, d_model)\n","        self.W_o = nn.Linear(d_model, d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask=None):\n","        batch_size, seq_len, _ = x.shape\n","\n","        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n","        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n","        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n","\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n","\n","        if mask is not None:\n","            scores = scores.masked_fill(mask == 0, float('-inf'))\n","\n","        attn = F.softmax(scores, dim=-1)\n","        attn = self.dropout(attn)\n","\n","        out = torch.matmul(attn, V)\n","        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n","        out = self.W_o(out)\n","\n","        return out\n","\n","# ========================================\n","# TRANSFORMER BLOCK\n","# ========================================\n","class TransformerBlock(nn.Module):\n","    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n","        super().__init__()\n","\n","        self.attn = MultiHeadAttention(d_model, num_heads, dropout)\n","        self.ff = nn.Sequential(\n","            nn.Linear(d_model, d_ff),\n","            nn.GELU(),\n","            nn.Linear(d_ff, d_model),\n","            nn.Dropout(dropout)\n","        )\n","\n","        self.ln1 = nn.LayerNorm(d_model)\n","        self.ln2 = nn.LayerNorm(d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        x = x + self.dropout(self.attn(self.ln1(x), mask))\n","        x = x + self.dropout(self.ff(self.ln2(x)))\n","        return x\n","\n","# ========================================\n","# GPT MODEL\n","# ========================================\n","class GPT(nn.Module):\n","    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff,\n","                 max_seq_len, dropout=0.1):\n","        super().__init__()\n","\n","        self.token_embedding = nn.Embedding(vocab_size, d_model)\n","        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n","\n","        self.layers = nn.ModuleList([\n","            TransformerBlock(d_model, num_heads, d_ff, dropout)\n","            for _ in range(num_layers)\n","        ])\n","\n","        self.ln_f = nn.LayerNorm(d_model)\n","        self.head = nn.Linear(d_model, vocab_size, bias=False)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, x):\n","        batch_size, seq_len = x.shape\n","\n","        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).unsqueeze(0).unsqueeze(0)\n","\n","        x = self.token_embedding(x)\n","        x = self.pos_encoding(x)\n","        x = self.dropout(x)\n","\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","\n","        x = self.ln_f(x)\n","        logits = self.head(x)\n","\n","        return logits\n","\n","# ========================================\n","# TRAINING FUNCTIONS\n","# ========================================\n","def train_epoch(model, loader, optimizer, scheduler, device):\n","    model.train()\n","    total_loss = 0\n","\n","    for batch_idx, (x, y) in enumerate(loader):\n","        x, y = x.to(device), y.to(device)\n","\n","        logits = model(x)\n","        loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","\n","        if batch_idx % 100 == 0:\n","            print(f\"  Batch {batch_idx}/{len(loader)}, Loss: {loss.item():.4f}\")\n","\n","    return total_loss / len(loader)\n","\n","def validate(model, loader, device):\n","    model.eval()\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(device), y.to(device)\n","            logits = model(x)\n","            loss = F.cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n","            total_loss += loss.item()\n","\n","    return total_loss / len(loader)\n","\n","@torch.no_grad()\n","def generate(model, prompt, max_new_tokens=100, temperature=1.0, top_k=None):\n","    model.eval()\n","\n","    tokens = torch.tensor([ord(c) for c in prompt], dtype=torch.long).unsqueeze(0).to(device)\n","\n","    for _ in range(max_new_tokens):\n","        tokens_cond = tokens if tokens.size(1) <= max_seq_len else tokens[:, -max_seq_len:]\n","\n","        logits = model(tokens_cond)\n","        logits = logits[:, -1, :] / temperature\n","\n","        if top_k is not None:\n","            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n","            logits[logits < v[:, [-1]]] = -float('Inf')\n","\n","        probs = F.softmax(logits, dim=-1)\n","        next_token = torch.multinomial(probs, num_samples=1)\n","\n","        tokens = torch.cat([tokens, next_token], dim=1)\n","\n","    generated = ''.join([chr(int(t)) for t in tokens[0].cpu().numpy()])\n","    return generated\n","\n","# ========================================\n","# MAIN TRAINING - CON CHECKPOINT SYSTEM\n","# ========================================\n","model = GPT(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout).to(device)\n","print(f\"Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95))\n","total_steps = len(train_loader) * num_epochs\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_steps, eta_min=learning_rate/10)\n","\n","# History\n","history = {'train_loss': [], 'val_loss': []}\n","best_val_loss = float('inf')\n","\n","# Training loop\n","print(\"\\nüöÄ Starting training...\")\n","for epoch in range(num_epochs):\n","    print(f\"\\n{'='*50}\")\n","    print(f\"Epoch {epoch+1}/{num_epochs}\")\n","    print('='*50)\n","\n","    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n","    val_loss = validate(model, val_loader, device)\n","\n","    print(f\"\\nüìä Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n","\n","    history['train_loss'].append(train_loss)\n","    history['val_loss'].append(val_loss)\n","\n","    # üíæ SALVA CHECKPOINT OGNI EPOCA\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'train_loss': train_loss,\n","        'val_loss': val_loss,\n","        'history': history\n","    }\n","\n","    # Salva checkpoint epoch\n","    checkpoint_path = f'{checkpoint_dir}/checkpoint_epoch_{epoch+1}.pth'\n","    torch.save(checkpoint, checkpoint_path)\n","    print(f\"üíæ Checkpoint salvato: checkpoint_epoch_{epoch+1}.pth\")\n","\n","    # Salva best model\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        best_path = f'{checkpoint_dir}/best_model.pth'\n","        torch.save(checkpoint, best_path)\n","        print(f\"‚≠ê BEST MODEL! Val Loss: {val_loss:.4f}\")\n","\n","    # Generate sample\n","    if (epoch + 1) % 2 == 0:\n","        print(f\"\\nüé≠ Generated sample:\")\n","        prompt = \"ROMEO:\"\n","        generated = generate(model, prompt, max_new_tokens=200, temperature=temperature, top_k=top_k)\n","        print(generated[:400])\n","\n","print(\"\\n‚úÖ Training completato!\")\n","\n","# Test finale\n","print(\"\\n\" + \"=\"*50)\n","print(\"üé≠ GENERAZIONE FINALE\")\n","print(\"=\"*50)\n","\n","prompts = [\"ROMEO:\", \"JULIET:\", \"KING LEAR:\"]\n","for prompt in prompts:\n","    print(f\"\\nüé¨ Prompt: {prompt}\")\n","    generated = generate(model, prompt, max_new_tokens=150, temperature=0.8, top_k=40)\n","    print(generated[:300])\n","    print(\"-\"*50)\n","\n","# Salva modello finale\n","final_path = f'{checkpoint_dir}/shakespeare_gpt_final.pth'\n","torch.save(model.state_dict(), final_path)\n","print(f\"\\nüíæ Modello finale salvato: shakespeare_gpt_final.pth\")\n","\n","# Scarica\n","from google.colab import files\n","print(\"\\nüì• Vuoi scaricare il modello? Esegui:\")\n","print(\"files.download(f'{checkpoint_dir}/best_model.pth')\")\n"]}]}